{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1. Install Library"
      ],
      "metadata": {
        "id": "rzRycXNGRY5F"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mcakjYVgRWsk",
        "outputId": "60175a67-3e0c-42c4-83f4-04de213db4c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting Sastrawi\n",
            "  Downloading Sastrawi-1.0.1-py2.py3-none-any.whl (209 kB)\n",
            "\u001b[K     |████████████████████████████████| 209 kB 13.6 MB/s \n",
            "\u001b[?25hInstalling collected packages: Sastrawi\n",
            "Successfully installed Sastrawi-1.0.1\n"
          ]
        }
      ],
      "source": [
        "pip install Sastrawi"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install swifter"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "TNW0mNrdVbjL",
        "outputId": "9f0f149f-c2c7-479f-d74f-3ea04254018b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting swifter\n",
            "  Downloading swifter-1.3.4.tar.gz (830 kB)\n",
            "\u001b[K     |████████████████████████████████| 830 kB 16.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from swifter) (1.3.5)\n",
            "Collecting psutil>=5.6.6\n",
            "  Downloading psutil-5.9.4-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (280 kB)\n",
            "\u001b[K     |████████████████████████████████| 280 kB 38.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: dask[dataframe]>=2.10.0 in /usr/local/lib/python3.8/dist-packages (from swifter) (2022.2.1)\n",
            "Requirement already satisfied: tqdm>=4.33.0 in /usr/local/lib/python3.8/dist-packages (from swifter) (4.64.1)\n",
            "Requirement already satisfied: ipywidgets>=7.0.0 in /usr/local/lib/python3.8/dist-packages (from swifter) (7.7.1)\n",
            "Requirement already satisfied: cloudpickle>=0.2.2 in /usr/local/lib/python3.8/dist-packages (from swifter) (1.5.0)\n",
            "Requirement already satisfied: parso>0.4.0 in /usr/local/lib/python3.8/dist-packages (from swifter) (0.8.3)\n",
            "Requirement already satisfied: bleach>=3.1.1 in /usr/local/lib/python3.8/dist-packages (from swifter) (5.0.1)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.8/dist-packages (from bleach>=3.1.1->swifter) (1.15.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.8/dist-packages (from bleach>=3.1.1->swifter) (0.5.1)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.8/dist-packages (from dask[dataframe]>=2.10.0->swifter) (6.0)\n",
            "Requirement already satisfied: fsspec>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from dask[dataframe]>=2.10.0->swifter) (2022.11.0)\n",
            "Requirement already satisfied: toolz>=0.8.2 in /usr/local/lib/python3.8/dist-packages (from dask[dataframe]>=2.10.0->swifter) (0.12.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from dask[dataframe]>=2.10.0->swifter) (21.3)\n",
            "Requirement already satisfied: partd>=0.3.10 in /usr/local/lib/python3.8/dist-packages (from dask[dataframe]>=2.10.0->swifter) (1.3.0)\n",
            "Requirement already satisfied: numpy>=1.18 in /usr/local/lib/python3.8/dist-packages (from dask[dataframe]>=2.10.0->swifter) (1.21.6)\n",
            "Requirement already satisfied: widgetsnbextension~=3.6.0 in /usr/local/lib/python3.8/dist-packages (from ipywidgets>=7.0.0->swifter) (3.6.1)\n",
            "Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.8/dist-packages (from ipywidgets>=7.0.0->swifter) (5.6.0)\n",
            "Requirement already satisfied: ipykernel>=4.5.1 in /usr/local/lib/python3.8/dist-packages (from ipywidgets>=7.0.0->swifter) (5.3.4)\n",
            "Requirement already satisfied: ipython>=4.0.0 in /usr/local/lib/python3.8/dist-packages (from ipywidgets>=7.0.0->swifter) (7.9.0)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from ipywidgets>=7.0.0->swifter) (3.0.3)\n",
            "Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.8/dist-packages (from ipywidgets>=7.0.0->swifter) (0.2.0)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.8/dist-packages (from ipykernel>=4.5.1->ipywidgets>=7.0.0->swifter) (6.1.12)\n",
            "Requirement already satisfied: tornado>=4.2 in /usr/local/lib/python3.8/dist-packages (from ipykernel>=4.5.1->ipywidgets>=7.0.0->swifter) (6.0.4)\n",
            "Requirement already satisfied: pexpect in /usr/local/lib/python3.8/dist-packages (from ipython>=4.0.0->ipywidgets>=7.0.0->swifter) (4.8.0)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.8/dist-packages (from ipython>=4.0.0->ipywidgets>=7.0.0->swifter) (0.7.5)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.8/dist-packages (from ipython>=4.0.0->ipywidgets>=7.0.0->swifter) (0.2.0)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.8/dist-packages (from ipython>=4.0.0->ipywidgets>=7.0.0->swifter) (2.6.1)\n",
            "Collecting jedi>=0.10\n",
            "  Downloading jedi-0.18.2-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6 MB 54.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.8/dist-packages (from ipython>=4.0.0->ipywidgets>=7.0.0->swifter) (57.4.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.8/dist-packages (from ipython>=4.0.0->ipywidgets>=7.0.0->swifter) (4.4.2)\n",
            "Requirement already satisfied: prompt-toolkit<2.1.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from ipython>=4.0.0->ipywidgets>=7.0.0->swifter) (2.0.10)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->dask[dataframe]>=2.10.0->swifter) (3.0.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas>=1.0.0->swifter) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas>=1.0.0->swifter) (2022.6)\n",
            "Requirement already satisfied: locket in /usr/local/lib/python3.8/dist-packages (from partd>=0.3.10->dask[dataframe]>=2.10.0->swifter) (1.0.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.8/dist-packages (from prompt-toolkit<2.1.0,>=2.0.0->ipython>=4.0.0->ipywidgets>=7.0.0->swifter) (0.2.5)\n",
            "Requirement already satisfied: notebook>=4.4.1 in /usr/local/lib/python3.8/dist-packages (from widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->swifter) (5.7.16)\n",
            "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.8/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->swifter) (0.15.0)\n",
            "Requirement already satisfied: nbconvert<6.0 in /usr/local/lib/python3.8/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->swifter) (5.6.1)\n",
            "Requirement already satisfied: terminado>=0.8.1 in /usr/local/lib/python3.8/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->swifter) (0.13.3)\n",
            "Requirement already satisfied: nbformat in /usr/local/lib/python3.8/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->swifter) (5.7.0)\n",
            "Requirement already satisfied: Send2Trash in /usr/local/lib/python3.8/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->swifter) (1.8.0)\n",
            "Requirement already satisfied: pyzmq>=17 in /usr/local/lib/python3.8/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->swifter) (23.2.1)\n",
            "Requirement already satisfied: jinja2<=3.0.0 in /usr/local/lib/python3.8/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->swifter) (2.11.3)\n",
            "Requirement already satisfied: jupyter-core>=4.4.0 in /usr/local/lib/python3.8/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->swifter) (5.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.8/dist-packages (from jinja2<=3.0.0->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->swifter) (2.0.1)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.8/dist-packages (from jupyter-core>=4.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->swifter) (2.5.4)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.8/dist-packages (from nbconvert<6.0->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->swifter) (1.5.0)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.8/dist-packages (from nbconvert<6.0->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->swifter) (0.8.4)\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.8/dist-packages (from nbconvert<6.0->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->swifter) (0.4)\n",
            "Requirement already satisfied: testpath in /usr/local/lib/python3.8/dist-packages (from nbconvert<6.0->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->swifter) (0.6.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.8/dist-packages (from nbconvert<6.0->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->swifter) (0.7.1)\n",
            "Requirement already satisfied: fastjsonschema in /usr/local/lib/python3.8/dist-packages (from nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->swifter) (2.16.2)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.8/dist-packages (from nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->swifter) (4.3.3)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.8/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->swifter) (22.1.0)\n",
            "Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.8/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->swifter) (5.10.0)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.8/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->swifter) (0.19.2)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.8/dist-packages (from importlib-resources>=1.4.0->jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->swifter) (3.11.0)\n",
            "Requirement already satisfied: ptyprocess in /usr/local/lib/python3.8/dist-packages (from terminado>=0.8.1->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->swifter) (0.7.0)\n",
            "Building wheels for collected packages: swifter\n",
            "  Building wheel for swifter (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for swifter: filename=swifter-1.3.4-py3-none-any.whl size=16321 sha256=44fa05be5a1e0bd46f369f877ce6b80bce8347ef6965d5f725ade38bc727fd16\n",
            "  Stored in directory: /root/.cache/pip/wheels/08/66/b4/921e351e63d88696932279d6163e125727c9da70ed8ca38419\n",
            "Successfully built swifter\n",
            "Installing collected packages: jedi, psutil, swifter\n",
            "  Attempting uninstall: psutil\n",
            "    Found existing installation: psutil 5.4.8\n",
            "    Uninstalling psutil-5.4.8:\n",
            "      Successfully uninstalled psutil-5.4.8\n",
            "Successfully installed jedi-0.18.2 psutil-5.9.4 swifter-1.3.4\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "psutil"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Import Library And Download"
      ],
      "metadata": {
        "id": "p7XY2NT2VlSy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd \n",
        "import numpy as np\n",
        "import nltk\n",
        "import string \n",
        "import re\n",
        "import swifter\n",
        "from nltk.tokenize import word_tokenize \n",
        "from nltk.probability import FreqDist\n",
        "from nltk.corpus import stopwords\n",
        "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
        "\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AtClU5UXVgeF",
        "outputId": "58088522-4016-45c4-cc2f-84824172e1f3"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Load Data And Put On Data Frame"
      ],
      "metadata": {
        "id": "a32KEOc2WKVm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "readJson = pd.read_json(\"banjirJakarta2022sept-nov.json\",orient='records', lines=True)\n",
        "\n",
        "TWEET_DATA = pd.DataFrame(readJson)\n",
        "TWEET_DATA.head(5)"
      ],
      "metadata": {
        "id": "QkWRyQ-GV0El"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Case Folding"
      ],
      "metadata": {
        "id": "274MXCoUWlPc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "TWEET_DATA['tweet'] = TWEET_DATA['text'].str.lower()\n",
        "TWEET_DATA.head(5)"
      ],
      "metadata": {
        "id": "vRZmHz3QWncp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Remove Special Character Include URL, Mention, Hashtag, Space, Punctuation, Single Char"
      ],
      "metadata": {
        "id": "qwF8jCAVWqyP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_tweet_special(text):\n",
        "    # remove tab, new line, ans back slice\n",
        "    text = text.replace('\\\\t',\" \").replace('\\\\n',\" \").replace('\\\\u',\" \").replace('\\\\',\"\")\n",
        "    # remove non ASCII (emoticon, chinese word, .etc)\n",
        "    text = text.encode('ascii', 'replace').decode('ascii')\n",
        "    # remove mention, link, hashtag\n",
        "    text = ' '.join(re.sub(\"([@#_][A-Za-z0-9]+)|(\\w+:\\/\\/\\S+)\",\" \", text).split())\n",
        "    # remove incomplete URL\n",
        "    return text.replace(\"http://\", \" \").replace(\"https://\", \" \")\n",
        "                \n",
        "TWEET_DATA['tweet'] = TWEET_DATA['tweet'].apply(remove_tweet_special)\n",
        "\n",
        "#remove number\n",
        "def remove_number(text):\n",
        "    return  re.sub(r\"\\d+\", \"\", text)\n",
        "\n",
        "TWEET_DATA['tweet'] = TWEET_DATA['tweet'].apply(remove_number)\n",
        "\n",
        "#remove punctuation\n",
        "def remove_punctuation(text):\n",
        "    return text.translate(str.maketrans(\"\",\"\",string.punctuation))\n",
        "\n",
        "TWEET_DATA['tweet'] = TWEET_DATA['tweet'].apply(remove_punctuation)\n",
        "\n",
        "#remove whitespace leading & trailing\n",
        "def remove_whitespace_LT(text):\n",
        "    return text.strip()\n",
        "\n",
        "TWEET_DATA['tweet'] = TWEET_DATA['tweet'].apply(remove_whitespace_LT)\n",
        "\n",
        "#remove multiple whitespace into single whitespace\n",
        "def remove_whitespace_multiple(text):\n",
        "    return re.sub('\\s+',' ',text)\n",
        "\n",
        "TWEET_DATA['tweet'] = TWEET_DATA['tweet'].apply(remove_whitespace_multiple)\n",
        "\n",
        "# remove single char\n",
        "def remove_singl_char(text):\n",
        "    return re.sub(r\"\\b[a-zA-Z]\\b\", \"\", text)\n",
        "\n",
        "TWEET_DATA['tweet'] = TWEET_DATA['tweet'].apply(remove_singl_char)\n",
        "\n",
        "TWEET_DATA.head(5)"
      ],
      "metadata": {
        "id": "z8IjHWSjWv9S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "6.Tokenizing"
      ],
      "metadata": {
        "id": "uRDoaIh-W7iL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def word_tokenize_wrapper(text):\n",
        "    return word_tokenize(text)\n",
        "\n",
        "TWEET_DATA['tweet'] = TWEET_DATA['tweet'].apply(word_tokenize_wrapper)\n",
        "\n",
        "TWEET_DATA.head(5)"
      ],
      "metadata": {
        "id": "f7LafZXaW9eF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Remove Stopwords"
      ],
      "metadata": {
        "id": "eq9cOZB3XB0q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------- get stopword from NLTK stopword -------------------------------\n",
        "# get stopword indonesia\n",
        "list_stopwords = stopwords.words('indonesian')\n",
        "\n",
        "# # ----------------------- add stopword from txt file ------------------------------------\n",
        "# # read txt stopword using pandas\n",
        "# txt_stopword = pd.read_csv(\"stopwords.txt\", names= [\"stopwords\"], header = None)\n",
        "\n",
        "# # convert stopword string to list & append additional stopword\n",
        "# list_stopwords.extend(txt_stopword[\"stopwords\"][0].split(' '))\n",
        "\n",
        "# # ---------------------------------------------------------------------------------------\n",
        "\n",
        "# convert list to dictionary\n",
        "list_stopwords = set(list_stopwords)\n",
        "\n",
        "\n",
        "#remove stopword pada list token\n",
        "def stopwords_removal(words):\n",
        "    return [word for word in words if word not in list_stopwords]\n",
        "\n",
        "TWEET_DATA['tweet'] = TWEET_DATA['tweet'].apply(stopwords_removal)\n",
        "TWEET_DATA.head(5)"
      ],
      "metadata": {
        "id": "qPIAaNeMXNFv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. Normalization"
      ],
      "metadata": {
        "id": "Jk5I1wOAXVi4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "normalizad_word = pd.read_excel(\"normalisasi.xlsx\")\n",
        "# TWEET_DATA.drop(columns=['tweet_normalized','tweet_tokens_stemmed'], axis=1,inplace=True)\n",
        "\n",
        "normalizad_word_dict = {}\n",
        "\n",
        "for index, row in normalizad_word.iterrows():\n",
        "    if row[0] not in normalizad_word_dict:\n",
        "        normalizad_word_dict[row[0]] = row[1] \n",
        "\n",
        "def normalized_term(document):\n",
        "    return [normalizad_word_dict[term] if term in normalizad_word_dict else term for term in document]\n",
        "\n",
        "TWEET_DATA['tweet'] = TWEET_DATA['tweet'].apply(normalized_term)\n",
        "\n",
        "TWEET_DATA.head(5)"
      ],
      "metadata": {
        "id": "U_cBCNviXXRS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. Stemming"
      ],
      "metadata": {
        "id": "wkYupFbAXbda"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create stemmer\n",
        "factory = StemmerFactory()\n",
        "stemmer = factory.create_stemmer()\n",
        "\n",
        "# stemmed\n",
        "def stemmed_wrapper(term):\n",
        "    return stemmer.stem(term)\n",
        "\n",
        "term_dict = {}\n",
        "\n",
        "for document in TWEET_DATA['tweet']:\n",
        "    for term in document:\n",
        "        if term not in term_dict:\n",
        "            term_dict[term] = ' '\n",
        "\n",
        "for term in term_dict:\n",
        "    term_dict[term] = stemmed_wrapper(term)\n",
        "\n",
        "\n",
        "# apply stemmed term to dataframe\n",
        "def get_stemmed_term(document):\n",
        "    return [term_dict[term] for term in document]\n",
        "\n",
        "TWEET_DATA['tweet'] = TWEET_DATA['tweet'].swifter.apply(get_stemmed_term)\n",
        "TWEET_DATA.head(5)"
      ],
      "metadata": {
        "id": "s5pj6AKAXc-c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. Labelling"
      ],
      "metadata": {
        "id": "emwJfKBcXnsy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "import json\n",
        "import reprlib\n",
        "nltk.download('vader_lexicon')\n",
        "\n",
        "# Memanfaatkan nltk VADER untuk menggunakan leksikon kustom\n",
        "sia1A, sia1B = SentimentIntensityAnalyzer(), SentimentIntensityAnalyzer()\n",
        "# membersihkan leksikon VADER default\n",
        "sia1A.lexicon.clear()\n",
        "sia1B.lexicon.clear()\n",
        "\n",
        "# Membaca leksikon InSet\n",
        "# Leksikon InSet lexicon dibagi menjadi dua, yakni polaritas negatif dan polaritas positif;\n",
        "# kita akan menggunakan nilai compound saja untuk memberi label pada suatu kalimat\n",
        "with open('dictset-neg.json') as f:\n",
        "    data1A = f.read()\n",
        "with open('dictset-pos.json') as f:\n",
        "    data1B = f.read()\n",
        "\n",
        "# Mengubah leksikon sebagai dictionary\n",
        "insetNeg = json.loads(data1A)\n",
        "insetPos = json.loads(data1B)\n",
        "\n",
        "# Update leksikon VADER yang sudah 'dimodifikasi'\n",
        "sia1A.lexicon.update(insetNeg)\n",
        "sia1B.lexicon.update(insetPos)"
      ],
      "metadata": {
        "id": "pgsXh2DFXqD4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def is_positive_inset(tweet: str) -> int:\n",
        "    \"\"\"True if tweet has positive compound sentiment, False otherwise.\"\"\"\n",
        "    return sia1A.polarity_scores(tweet)[\"compound\"] + sia1B.polarity_scores(tweet)[\"compound\"] > 0"
      ],
      "metadata": {
        "id": "1gkLeu35Xyf4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TWEET_DATA[\"textjoin\"] = TWEET_DATA[\"tweet\"].apply(lambda x : \" \".join(x))\n",
        "tweets = TWEET_DATA[\"textjoin\"]\n",
        "\n",
        "for ind in range(len(TWEET_DATA)):\n",
        "  if is_positive_inset(TWEET_DATA.loc[ind,\"textjoin\"]) == True:\n",
        "    TWEET_DATA.loc[ind,\"sentimen\"] = 'Positif'\n",
        "  else:\n",
        "    TWEET_DATA.loc[ind,\"sentimen\"] = 'Negatif'\n",
        "\n",
        "\n",
        "TWEET_DATA"
      ],
      "metadata": {
        "id": "H_wZau6pX67l"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}